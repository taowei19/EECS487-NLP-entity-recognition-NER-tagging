{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example\n",
    "\n",
    "This notebook walks through a toy example demonstrating the correct outputs for several functions from within `hmm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from hmm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "This toy example is based on the textbook notes for hidden Markov models, which can be accessed using [this link](https://web.stanford.edu/~jurafsky/slp3/A.pdf). Please note: the probability values (initial, transition, and emission) are NOT the same as in the textbook.\n",
    "\n",
    "In the example they provide, our task is to create a hidden Markov model that relates the number of ice creams eaten by a man named Jason to the weather on a given day. The weather can be either \"cold\" or \"hot\"--these will be our hidden states. Jason eats either 1, 2, or 3 ice creams every day--these will be our observations.\n",
    "\n",
    "Some example data has been loaded into `toy_example.txt`. The following code loads this example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3\n",
      "Number of tokens: 19\n",
      "Train data id 0: [['1', 'cold'], ['2', 'cold'], ['2', 'hot'], ['3', 'hot'], ['1', 'cold'], ['</s>', '<end>']]\n",
      "Train data id 0: [['3', 'hot'], ['3', 'hot'], ['2', 'cold'], ['1', 'cold'], ['2', 'hot'], ['</s>', '<end>']]\n",
      "Train data id 0: [['1', 'cold'], ['3', 'hot'], ['2', 'hot'], ['2', 'cold'], ['2', 'hot'], ['3', 'hot'], ['</s>', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "data = load_data('toy_example.txt')\n",
    "print(f\"Number of sentences: {len(data)}\")\n",
    "print(f\"Number of tokens: {sum([len(i) for i in data])}\")\n",
    "print(f\"Train data id 0: {data[0]}\")\n",
    "print(f\"Train data id 0: {data[1]}\")\n",
    "print(f\"Train data id 0: {data[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frequency Count\n",
    "\n",
    "Now we need to program `get_counts`. This function should take as its input `data` and compute the properties `initial_count`, `transition_count`, and `emission_count`. \n",
    "\n",
    "Of course, we are not performing NER tagging for this toy example, but we can use `HMMNER` anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: {'cold': 2, 'hot': 1}\n",
      "Transition count: {('cold', 'cold'): 2, ('cold', 'hot'): 4, ('hot', 'hot'): 4, ('hot', 'cold'): 3, ('cold', '<end>'): 1, ('hot', '<end>'): 2}\n",
      "Emission count: {('cold', '1'): 4, ('cold', '2'): 3, ('hot', '2'): 4, ('hot', '3'): 5, ('<end>', '</s>'): 3}\n"
     ]
    }
   ],
   "source": [
    "hmm = HMMNER()\n",
    "hmm.get_counts(data)\n",
    "\n",
    "print(f\"Initial count: {hmm.initial_count}\")\n",
    "print(f\"Transition count: {hmm.transition_count}\")\n",
    "print(f\"Emission count: {hmm.emission_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Tags and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER tags: ['<end>', 'cold', 'hot']\n",
      "Index of cold: 1\n",
      "All observations: ['1', '2', '3', '</s>']\n"
     ]
    }
   ],
   "source": [
    "hmm.get_lists()\n",
    "print(f\"NER tags: {hmm.ner_tags}\")\n",
    "print(f\"Index of cold: {hmm.tag_to_index['cold']}\")\n",
    "print(f\"All observations: {hmm.observations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Probabilities\n",
    "\n",
    "In this cell we need to convert the raw frequency counts into probability values. We can use a relatively simple procedure for add-k smoothing:\n",
    "\n",
    "- Calculate the probability values without smoothing\n",
    "- Add k to each probability value\n",
    "- Normalize the entire distribution (make sure it sums to 1)\n",
    "\n",
    "This process should be done separately for the `initial_prob`, `transition_prob`, and `emission_prob` arrays. \n",
    "\n",
    "One question that may arise is: across which axis/dimension should the normalization occur? Of course, since `initial_prob` is one-dimensional, normalization should take place along that one dimension. For `transition_prob` and `emission_prob`, both of which are two-dimensional, normalization should occur along each row (`axis=1`). The reason is that the formulae we use (for both the Viterbi/Beam Search and Forward algorithms) utilize conditional probability distributions described by each row. For instance, the transition probabilities are modeled as $\\Pr(t_i | t_{i-1})$, where $t_i$ is the i'th tag. The distribution over values of $t_i$ is a single row in `transition_prob`. Thus, it should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prob:\n",
      "[0.03030303 0.63636364 0.33333333]\n",
      "1.0\n",
      "Initial prob of 'cold': 0.6363636363636364\n",
      "\n",
      "Transition prob:\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.15068493 0.28767123 0.56164384]\n",
      " [0.22580645 0.33333333 0.44086022]]\n",
      "[1. 1. 1.]\n",
      "Transition prob from 'cold' to 'hot': 0.5616438356164383\n",
      "\n",
      "Emission prob:\n",
      "[[0.02941176 0.02941176 0.02941176 0.91176471]\n",
      " [0.55405405 0.41891892 0.01351351 0.01351351]\n",
      " [0.0106383  0.43617021 0.54255319 0.0106383 ]]\n",
      "[1. 1. 1.]\n",
      "Emission prob from 'cold' to '1': 0.5540540540540541\n"
     ]
    }
   ],
   "source": [
    "initial_k, transition_k, emission_k = 0.1, 0.1, 0.1\n",
    "\n",
    "hmm.get_probabilities(initial_k, transition_k, emission_k)\n",
    "\n",
    "print(f\"Initial prob:\\n{hmm.initial_prob}\")\n",
    "print(hmm.initial_prob.sum())\n",
    "print(f\"Initial prob of 'cold': {hmm.initial_prob[hmm.tag_to_index['cold']]}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Transition prob:\\n{hmm.transition_prob}\")\n",
    "print(hmm.transition_prob.sum(axis=1))\n",
    "print(f\"Transition prob from 'cold' to 'hot': {hmm.transition_prob[hmm.tag_to_index['cold'], hmm.tag_to_index['hot']]}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Emission prob:\\n{hmm.emission_prob}\")\n",
    "print(hmm.emission_prob.sum(axis=1))\n",
    "print(f\"Emission prob from 'cold' to '1': {hmm.emission_prob[hmm.tag_to_index['cold'], hmm.observation_to_index['1']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beam Search\n",
    "\n",
    "Now we need to implement beam search. The output displayed below provides a concrete example of the `tags` and `backtrace` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Index Matrix:\n",
      " [[1 2 2 2 1 0]\n",
      " [2 0 0 1 0 2]]\n",
      "Backtrace Matrix:\n",
      " [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "['cold', 'hot', 'hot', 'hot', 'cold', '<end>']\n"
     ]
    }
   ],
   "source": [
    "beam_width = 2\n",
    "\n",
    "sample_data = ['1', '3', '3', '2', '1', '</s>']\n",
    "weather_tags = hmm.beam_search(sample_data, beam_width, should_print=True)\n",
    "print(weather_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis purposes, I have copied the `tags` and `backtrace` matrices below.\n",
    "\n",
    "In this instance `tags[0,0] = 2`. To find out which tag this refers to, we can use `hmm.ner_tags`.\n",
    "\n",
    "Now suppose we want to know which tag provided the maximum probability value for `tags[0,2]` (which tag preceded `tags[0,2]`). To do this, we can use the same indices to index into `backtrace`. Refer to the code below to see the concrete implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags[0,0] = 2 refers to 'hot'\n",
      "The tag in tags[0,2] was preceded by 'hot'\n"
     ]
    }
   ],
   "source": [
    "tags = np.array([[2, 0, 0, 1, 0], [1, 2, 2, 2, 1]])\n",
    "backtrace = np.array([[0, 1, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# Which tag does tags[0,0] refer to?\n",
    "this_tag = hmm.ner_tags[tags[0,0]]\n",
    "print(f\"tags[0,0] = 2 refers to '{this_tag}'\")\n",
    "\n",
    "# Which tag provided the maximum probability value for tags[2,0]?\n",
    "index_of_prev_tag = backtrace[0,2]\n",
    "prev_tag_idx = tags[index_of_prev_tag, 2-1]\n",
    "prev_tag = hmm.ner_tags[prev_tag_idx]\n",
    "print(f\"The tag in tags[0,2] was preceded by '{prev_tag}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Forward Algorithm\n",
    "\n",
    "Now we need to implement the forward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.15068493 0.22580645]\n",
      "[0.33333333 0.28767123 0.33333333]\n",
      "[0.33333333 0.56164384 0.44086022]\n",
      "[0.33333333 0.15068493 0.22580645]\n",
      "[0.33333333 0.28767123 0.33333333]\n",
      "[0.33333333 0.56164384 0.44086022]\n",
      "[0.33333333 0.15068493 0.22580645]\n",
      "[0.33333333 0.28767123 0.33333333]\n",
      "[0.33333333 0.56164384 0.44086022]\n",
      "[0.33333333 0.15068493 0.22580645]\n",
      "[0.33333333 0.28767123 0.33333333]\n",
      "[0.33333333 0.56164384 0.44086022]\n",
      "[0.33333333 0.15068493 0.22580645]\n",
      "[0.33333333 0.28767123 0.33333333]\n",
      "[0.33333333 0.56164384 0.44086022]\n",
      "Log Probability Matrix:\n",
      " [[ -7  -6  -7  -8  -9  -8]\n",
      " [ -1  -6  -7  -5  -6 -11]\n",
      " [ -5  -2  -3  -5  -9 -11]]\n",
      "Log Probability: -8.218983217556692\n"
     ]
    }
   ],
   "source": [
    "sample_data = ['1', '3', '3', '2', '1', '</s>']\n",
    "prob = hmm.forward_algorithm(sample_data)\n",
    "print(f\"Log Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix/code structure for this algorithm is much simpler to understand than the code structure for beam search. For demonstration purposes, I have re-pasted the matrix below.\n",
    "\n",
    "The log-probability matrix is of shape `(num_tags, num_tokens_in_sentence)`. Each row refers to a single tag, and each column refers to a single token. Thus, `log_prob_mat[i,j]` refers to the log-probability that token `j` has tag `hmm.ner_tags[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a log-probability of -5 that the token '2' has the tag 'cold'\n"
     ]
    }
   ],
   "source": [
    "log_prob_mat = np.array([[-7, -6, -7, -8, -9], [-1, -6, -7, -5, -6], [-5, -2, -3, -5, -9]])\n",
    "\n",
    "sample_idx = [1,3]\n",
    "token = sample_data[sample_idx[1]]\n",
    "tag = hmm.ner_tags[sample_idx[0]]\n",
    "log_prob = log_prob_mat[sample_idx[0], sample_idx[1]]\n",
    "print(f\"There is a log-probability of {log_prob} that the token '{token}' has the tag '{tag}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
